{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phodara/SofGan/blob/main/Copy_of_Sofgan_renderer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UZRZvohjwE2H",
      "metadata": {
        "id": "UZRZvohjwE2H"
      },
      "source": [
        "#### This is the official Colab demo for the project https://apchenstu.github.io/sofgan/ \n",
        "#### Please make a copy if you wish to change the content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vcM9WpsjyELk",
      "metadata": {
        "id": "vcM9WpsjyELk"
      },
      "outputs": [],
      "source": [
        "# make sure you have one GPU allocated\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wFYD6bqFyR0A",
      "metadata": {
        "id": "wFYD6bqFyR0A"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UWlP6KDKvudv",
      "metadata": {
        "id": "UWlP6KDKvudv"
      },
      "outputs": [],
      "source": [
        "# ref: https://colab.research.google.com/github/dvschultz/stylegan2-ada-pytorch/blob/main/SG2_ADA_PyTorch.ipynb#scrollTo=B8ADVNpBh8Ox\n",
        "import os\n",
        "path_parent_folder = \"/content/drive/MyDrive/sofganColab\"\n",
        "path_root_folder = os.path.join(path_parent_folder, \"sofgan\")\n",
        "if os.path.isdir(path_parent_folder):\n",
        "    %cd $path_root_folder\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !mkdir sofganColab\n",
        "    %cd sofganColab\n",
        "    !git clone https://github.com/apchenstu/sofgan.git --recursive\n",
        "    %cd sofgan\n",
        "else:\n",
        "    !git clone https://github.com/apchenstu/sofgan.git --recursive\n",
        "    %cd sofgan\n",
        "\n",
        "!pip install ninja opensimplex torch==1.7.1 torchvision==0.8.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9aMjVf2-kbQ",
      "metadata": {
        "id": "d9aMjVf2-kbQ"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g_XrinaYqQoS",
      "metadata": {
        "id": "g_XrinaYqQoS"
      },
      "outputs": [],
      "source": [
        "import cv2, random\n",
        "import torch, os, argparse\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy.stats import norm\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms, utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "from modules.model_seg_input import Generator\n",
        "from modules.BiSeNet import BiSeNet\n",
        "from utils import *\n",
        "from modules.model_seg_input import scatter as scatter_model\n",
        "\n",
        "import sys,os\n",
        "root = os.path.abspath('.')\n",
        "os.chdir(root)\n",
        "sys.path.append(root)\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "torch.cuda.set_device(0)\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZED0axO86cK6",
      "metadata": {
        "id": "ZED0axO86cK6"
      },
      "outputs": [],
      "source": [
        "# should have fused*.o file\n",
        "!ls /root/.cache/torch_extensions/fused/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DJ_sC0yXqQoV",
      "metadata": {
        "id": "DJ_sC0yXqQoV"
      },
      "outputs": [],
      "source": [
        "\n",
        "IDList = [np.arange(17).tolist(),[0],[1,4,5,9,12],[15],[6,7,8,3],[11,13,14,16,10]]\n",
        "# IDList = [[0],[1,4,5,9,12],[15],[2,3,6,7,8,10,11,13,14,16]]\n",
        "groupName = ['Global','Background','Complexion','Hair','Eyes & Mouth','Wearings']\n",
        "def scatter_to_mask(segementation, out_num=1,add_whole=True,add_flip=False,region=None):\n",
        "    segementation = scatter_model(segementation)\n",
        "    masks = []\n",
        "\n",
        "    if None == region:\n",
        "        if add_whole:\n",
        "            mask = torch.sum(segementation, dim=1, keepdim=True).clamp(0.0, 1.0)\n",
        "            masks.append(torch.cat((mask, 1.0 - mask), dim=1))\n",
        "        if add_flip:\n",
        "            masks.append(torch.cat((1.0 - mask, mask), dim=1))\n",
        "\n",
        "\n",
        "        for i in range(out_num - add_whole - add_flip):\n",
        "            idList = IDList[i]\n",
        "            mask = torch.sum(segementation[:, idList], dim=1, keepdim=True).clamp(0.0, 1.0)\n",
        "            masks.append(torch.cat((1.0 - mask, mask), dim=1))\n",
        "    else:\n",
        "        for item in region:\n",
        "            idList = IDList[item]\n",
        "            mask = torch.sum(segementation[:, idList], dim=1, keepdim=True).clamp(0.0, 1.0)\n",
        "            masks.append(torch.cat((1.0 - mask, mask), dim=1))\n",
        "    masks = torch.cat(masks, dim=0)\n",
        "    return masks\n",
        "\n",
        "def make_noise(batch, styles_dim, style_repeat, latent_dim, n_noise, device):\n",
        "    noises = torch.randn(n_noise, batch, styles_dim, latent_dim, device=device).repeat(1, 1, style_repeat, 1)\n",
        "    return noises\n",
        "\n",
        "def mixing_noise(batch, latent_dim, prob, device, unbine=True):\n",
        "    n_noise = 1\n",
        "    style_dim = 2 if random.random() < prob else 1\n",
        "    style_repeat = 2 // style_dim  # if prob>0 else 1\n",
        "    styles = make_noise(batch, style_dim, style_repeat, latent_dim, n_noise, device)\n",
        "    return styles.unbind(0) if unbine else styles\n",
        "\n",
        "\n",
        "def sample_styles_with_miou(seg_label, num_style, mixstyle=0, truncation=0.9, batch_size=4, descending=False):\n",
        "    times = 0\n",
        "    in_batch = seg_label.shape[0]\n",
        "    if in_batch == 1:\n",
        "        batch = batch_size\n",
        "        seg_label = seg_label.repeat(batch, 1, 1, 1)\n",
        "    else:\n",
        "        batch = in_batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        styles_miou, count, mious = [], 0, []\n",
        "        while count < num_style:\n",
        "            styles = mixing_noise(batch // in_batch, args.latent, mixstyle, device, unbine=False)\n",
        "            styles = to_w_style(generator.style_map_norepeat, styles, latent_av, trunc_psi=truncation)\n",
        "            styles = torch.cat(styles, dim=0)\n",
        "            w_latent = generator.style_map([styles], to_w_space=False)\n",
        "\n",
        "            if in_batch > 1:\n",
        "                w_latent = w_latent.repeat(batch, 1, 1)\n",
        "\n",
        "            img, _, _, _ = generator(w_latent, return_latents=False, condition_img=seg_label, input_is_latent=True,\n",
        "                                     noise=noise)\n",
        "            img = img.clamp(-1.0, 1.0)\n",
        "            img = F.interpolate(img, size=(512, 512), mode='bilinear')\n",
        "\n",
        "            segmap = bisNet(img)[0]\n",
        "            segmap = F.interpolate(segmap, size=seg_label.shape[2:], mode='bilinear')\n",
        "            segmap = id_remap(torch.argmax(segmap, dim=1, keepdim=True))\n",
        "\n",
        "            thread = 0.46\n",
        "            if times > 15:\n",
        "                thread = 0.42\n",
        "            if times > 20:\n",
        "                thread = 0.35\n",
        "            if times > 30:\n",
        "                thread = 0.\n",
        "\n",
        "            miou = mIOU(segmap, seg_label)\n",
        "            miou = miou.min() if in_batch > 1 else miou\n",
        "            mask = (miou > thread).tolist()\n",
        "\n",
        "            times += 1\n",
        "            if np.sum(mask) == 0:\n",
        "                continue\n",
        "\n",
        "            if in_batch > 1 and mask:\n",
        "                mious.append(miou.view(-1, 1))\n",
        "                styles_miou.append(w_latent[[0]])\n",
        "                count += 1\n",
        "            else:\n",
        "                mious.append(miou[mask])\n",
        "                if len(mask) == w_latent.shape[0]:\n",
        "                    styles_miou.append(w_latent[mask])\n",
        "                else:\n",
        "                    styles_miou.append(\n",
        "                        w_latent.view(-1, 2, w_latent.shape[-2], w_latent.shape[-1])[mask])  # old need this\n",
        "                count += np.sum(mask)\n",
        "\n",
        "    mious = torch.cat(mious, dim=0).view(-1)\n",
        "    mious, indices = torch.sort(mious, descending=descending)\n",
        "    styles_miou = torch.cat(styles_miou, dim=0)[indices]\n",
        "    return styles_miou[:num_style]\n",
        "\n",
        "def initFaceParsing(n_classes=20):\n",
        "    net = BiSeNet(n_classes=n_classes)\n",
        "    net.cuda()\n",
        "    net.load_state_dict(torch.load('./ckpts/segNet-20Class.pth'))\n",
        "    net.eval()\n",
        "    to_tensor = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "\n",
        "    ])\n",
        "    return net, to_tensor\n",
        "\n",
        "\n",
        "def parsing_img(bisNet, image, to_tensor, argmax=True):\n",
        "    with torch.no_grad():\n",
        "        img = to_tensor(image)\n",
        "        img = torch.unsqueeze(img, 0).cuda()\n",
        "        segmap = bisNet(img)[0]\n",
        "        if argmax:\n",
        "            segmap = segmap.argmax(1, keepdim=True)\n",
        "        segmap = id_remap(segmap)\n",
        "    return img, segmap\n",
        "\n",
        "\n",
        "def auto_crop_img(image, detector=None, inv_pad=2):\n",
        "    if detector is None:\n",
        "        detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    dets = detector(image, 1)\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    faces = []\n",
        "    for i, d in enumerate(dets):\n",
        "        left, right, top, bottom = d.left(), d.right(), d.top(), d.bottom()\n",
        "        width_crop = right - left\n",
        "        pad = min(w - right, left, top, h - bottom, width_crop // inv_pad)\n",
        "\n",
        "        top = max(top - int(pad * 1.5), 0)\n",
        "        faces.append(image[top:top + width_crop + 2 * pad, left - pad:right + pad])\n",
        "    return faces\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3qsdO0ThqQoW",
      "metadata": {
        "id": "3qsdO0ThqQoW"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-i', '--input', type=str)\n",
        "parser.add_argument('-o', '--output', type=str)\n",
        "parser.add_argument('-batch_size', type=int,default=4)\n",
        "parser.add_argument('--resolution', type=int, default=1024)\n",
        "parser.add_argument('--nrows', type=int, default=6)\n",
        "parser.add_argument('--ckpt', type=str, default=None)\n",
        "parser.add_argument('--channel_multiplier', type=int, default=2)\n",
        "parser.add_argument('--with_rgb_input', action='store_true')\n",
        "parser.add_argument('--with_local_style', action='store_true')\n",
        "parser.add_argument('--condition_dim', type=int, default=0)\n",
        "parser.add_argument('--styles_path', type=str, default=None)\n",
        "parser.add_argument('--MODE', type=int, default=0)\n",
        "parser.add_argument('--miou_filter', action='store_true')\n",
        "parser.add_argument('--truncation', type=float, default=0.7)\n",
        "parser.add_argument('--with_seg_fc', action='store_true')\n",
        "\n",
        "cmd = f'-i ./dataset/video -o ./result/mv-obama/ \\\n",
        "--ckpt ./ckpts/generator.pt \\\n",
        "--resolution 1024  --MODE 2 --miou_filter --truncation 0.7'\n",
        "args = parser.parse_args(cmd.split())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vto4fyKwVYLd",
      "metadata": {
        "id": "vto4fyKwVYLd"
      },
      "source": [
        "Install checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d_Uw4gf2VRV_",
      "metadata": {
        "id": "d_Uw4gf2VRV_"
      },
      "outputs": [],
      "source": [
        "# TODO download checkpoint to gdrive\n",
        "# Download the https://drive.google.com/file/d/1LPKU3AJVlhnyXBGzLS0UrOEhIT1gcFpD/view?usp=sharing\n",
        "!pip install gdown\n",
        "# !gdown https://drive.google.com/file/d/1LPKU3AJVlhnyXBGzLS0UrOEhIT1gcFpD/view?usp=sharing\n",
        "!gdown https://drive.google.com/file/d/17SW3MurX_78_CfT29DaBZuqnKEjkbzQv/view?usp=sharing\n",
        "# Unzip at Root folder\n",
        "!unzip -fq ckpts.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MGuL1ZJUqQoX",
      "metadata": {
        "id": "MGuL1ZJUqQoX"
      },
      "outputs": [],
      "source": [
        "# define networks\n",
        "args.latent = 512\n",
        "args.n_mlp = 8\n",
        "args.condition_path = args.input\n",
        "generator = Generator(args).eval().to(device)\n",
        "\n",
        "ckpt = torch.load(args.ckpt)\n",
        "generator.load_state_dict(ckpt['g_ema'])\n",
        "\n",
        "batch_size = 4\n",
        "latent_av = cal_av(generator, batch_size, args.latent)\n",
        "\n",
        "# face parser\n",
        "bisNet, to_tensor = initFaceParsing()\n",
        "\n",
        "del ckpt\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TPnWWbOoqQoX",
      "metadata": {
        "id": "TPnWWbOoqQoX"
      },
      "source": [
        "# image style transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v8s14v_PqQoX",
      "metadata": {
        "id": "v8s14v_PqQoX"
      },
      "outputs": [],
      "source": [
        "img_path = './example/Harry.jpg'# path to the source image folder\n",
        "save_path = './example/test.png'\n",
        "auto_crop = False # you need to center crop the image if you are using your own photos; please set false if image comes from FFHQ or CelebA\n",
        "miou_filter = True # set true if you want to filter style with the miou\n",
        "n_styles = 3\n",
        "resolution_vis = 1024 # image resolution to save \n",
        "save_as_video = True\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    noise = [getattr(generator.noises, f'noise_{i}') for i in range(generator.num_layers)]\n",
        "\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    if auto_crop:\n",
        "        import dlib\n",
        "        faces = auto_crop_img(np.array(img))\n",
        "        img = Image.fromarray(faces[0])\n",
        "        \n",
        "    img, seg_label = parsing_img(bisNet, img.resize((512, 512)), to_tensor)\n",
        "    seg_label_rgb = vis_condition_img(seg_label)\n",
        "    seg_label_rgb = F.interpolate(seg_label_rgb, (args.resolution, args.resolution), mode='bilinear', align_corners=True)\n",
        "\n",
        "    try:\n",
        "        tqdm._instances.clear() \n",
        "    except Exception:     \n",
        "        pass\n",
        "        \n",
        "    if not save_as_video:\n",
        "        if miou_filter:\n",
        "            w_latent = sample_styles_with_miou(seg_label, n_styles * 2, mixstyle=mixstyle,\n",
        "                                           truncation=args.truncation, batch_size=args.batch_size)\n",
        "        else:\n",
        "            mixstyle = 0.0\n",
        "            styles = mixing_noise(n_styles, args.latent, mixstyle, device, unbine=False)\n",
        "            styles = to_w_style(generator.style_map_norepeat, styles, latent_av, trunc_psi=args.truncation)\n",
        "            styles = torch.cat(styles, dim=0)\n",
        "            w_latent = generator.style_map([styles], to_w_space=False)\n",
        "            w_latent = w_latent.view(-1,2,w_latent.shape[-2],w_latent.shape[-1])\n",
        "    \n",
        "        result = [F.interpolate(img.cpu(),(resolution_vis,resolution_vis), mode='bilinear', align_corners=True)]\n",
        "        for j in tqdm(range(n_styles)):\n",
        "            fake_img, _, _, _ = generator(w_latent[j], return_latents=False, condition_img=seg_label, \\\n",
        "                                          input_is_latent=True, noise=noise)\n",
        "            result.append(F.interpolate(fake_img.detach().cpu().clamp(-1.0, 1.0),(resolution_vis,resolution_vis), \n",
        "                                        mode='bilinear', align_corners=True))\n",
        "\n",
        "        result = torch.cat(result, dim=0)\n",
        "        utils.save_image(result, save_path,nrow=n_styles+1,normalize=True,range=(-1, 1),padding = 2)\n",
        "    else:\n",
        "        nrows,ncols = 1, 2\n",
        "        width_pad, height_pad = 2 * (ncols + 1), 2 * (nrows + 1) \n",
        "        fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
        "        out = cv2.VideoWriter(f'{save_path[:-4]}.mp4', fourcc,\n",
        "                              20, (resolution_vis * ncols + width_pad, resolution_vis * nrows + height_pad))\n",
        "\n",
        "        img = F.interpolate(img.cpu(),(resolution_vis,resolution_vis), mode='bilinear', align_corners=True)\n",
        "        w_latents = sample_styles_with_miou(seg_label, n_styles, mixstyle=0.0, truncation=args.truncation, batch_size=args.batch_size,descending=True)[:,0]\n",
        "        style_masks = scatter_to_mask(seg_label, len(groupName), add_flip=False, add_whole=False)\n",
        "        \n",
        "        w_latent_nexts = []\n",
        "        for i_style in tqdm(range(len(groupName))):\n",
        "\n",
        "            regions = list(range(n_styles)) + [0]\n",
        "\n",
        "            for j,frame in enumerate(range(1,len(regions))):\n",
        "\n",
        "                if 0 == regions[frame - 1]: # first style\n",
        "                    w_latent_last, w_latent_next = w_latents[:1], w_latents[[frame]]\n",
        "                elif 0 == regions[frame]:# last style\n",
        "                    w_latent_last, w_latent_next = w_latent_next.clone(), w_latents[:1]\n",
        "                else:\n",
        "                    w_latent_last = w_latent_next.clone()\n",
        "                    w_latent_next = w_latents[[frame]].clone()\n",
        "\n",
        "\n",
        "                frame_sub_count = 40 if i_style<4 else 30\n",
        "                cdf_scale = 1.0 / (1.0 - norm.cdf(-frame_sub_count // 2, 0, 6) * 2)\n",
        "                for frame_sub in range(-frame_sub_count // 2, frame_sub_count // 2 + 1):\n",
        "\n",
        "                    weight = (norm.cdf(frame_sub, 0, 6) - norm.cdf(-frame_sub_count // 2, 0, 6)) * cdf_scale\n",
        "\n",
        "                    w_latent_current = (1.0 - weight) * w_latent_last + weight * w_latent_next\n",
        "                    w_latent_current = torch.cat((w_latents[:1],w_latent_current),dim=0)\n",
        "\n",
        "\n",
        "                    # first row\n",
        "                    result = [img]\n",
        "                    w_latent_current_in = w_latent_current.view(-1, 18, 512)\n",
        "                    fake_img, _, _, _ = generator(w_latent_current_in, return_latents=False,\n",
        "                                                  condition_img=seg_label, \\\n",
        "                                                  input_is_latent=True, noise=noise,\n",
        "                                                  style_mask=style_masks[[i_style]])\n",
        "                    result.append(F.interpolate(fake_img.detach().cpu().clamp(-1.0, 1.0),(resolution_vis,resolution_vis)\n",
        "                                               , mode='bilinear', align_corners=True))\n",
        "\n",
        "                    result = torch.cat(result, dim=0)\n",
        "                    result = (utils.make_grid(result, nrow=ncols) + 1) / 2 * 255\n",
        "                    result = (result.detach().numpy()[::-1]).transpose((1, 2, 0))\n",
        "                    out.write(result.astype('uint8'))\n",
        "        out.release()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ALlLJ9Ka4OmL",
      "metadata": {
        "id": "ALlLJ9Ka4OmL"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('./example/test.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2HByU726qQoZ",
      "metadata": {
        "id": "2HByU726qQoZ"
      },
      "source": [
        "# video style transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hnYh4KfDqQoZ",
      "metadata": {
        "id": "hnYh4KfDqQoZ"
      },
      "outputs": [],
      "source": [
        "video_path = './example/faceCap.avi'# path to the source image folder\n",
        "save_path = './example/faceCap-restyle.mp4'\n",
        "auto_crop = False # you need to center crop the image if you are using your own photos; please set false if image is from FFHQ or CelebA\n",
        "resolution_vis = 512 # image resolution to save \n",
        "save_as_video = True\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "nrows,ncols = 2, 2\n",
        "width_pad, height_pad = 2 * (ncols + 1), 2 * (nrows + 1) \n",
        "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
        "out = cv2.VideoWriter(f'{save_path[:-4]}.mp4', fourcc,\n",
        "                      20, (resolution_vis * ncols + width_pad, resolution_vis * nrows + height_pad))\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    noise = [getattr(generator.noises, f'noise_{i}') for i in range(generator.num_layers)]\n",
        "    \n",
        "    mixstyle = 0.0\n",
        "    styles = mixing_noise(nrows*ncols, args.latent, mixstyle, device, unbine=False)\n",
        "    styles = to_w_style(generator.style_map_norepeat, styles, latent_av, trunc_psi=args.truncation)\n",
        "    styles = torch.cat(styles, dim=0)\n",
        "    w_latent = generator.style_map([styles], to_w_space=False)\n",
        "    w_latent = w_latent.view(-1,2,w_latent.shape[-2],w_latent.shape[-1])\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    success, img = cap.read()\n",
        "    \n",
        "    try:\n",
        "        tqdm._instances.clear() \n",
        "    except Exception:     \n",
        "        pass\n",
        "    for _ in tqdm(range(length-1)):\n",
        "\n",
        "        if auto_crop:\n",
        "            import dlib\n",
        "            faces = auto_crop_img(img[...,::-1])#bgr -> rgb\n",
        "            img = Image.fromarray(faces[0])\n",
        "        else:\n",
        "            img = Image.fromarray(img[...,::-1])\n",
        "\n",
        "        # you may need a facial landmark detector to remove the jitter on the eyes region\n",
        "        # we provide a more stable video parser, you can find it in the readme.\n",
        "        img, seg_label = parsing_img(bisNet, img.resize((512, 512)), to_tensor)\n",
        "        seg_label_rgb = vis_condition_img(seg_label)\n",
        "        seg_label_rgb = F.interpolate(seg_label_rgb, (args.resolution, args.resolution), mode='bilinear', align_corners=True)\n",
        "        \n",
        "\n",
        "        result = [F.interpolate(img.cpu(),(resolution_vis,resolution_vis))]\n",
        "        for j in range(nrows*ncols-1):\n",
        "            fake_img, _, _, _ = generator(w_latent[j], return_latents=False, condition_img=seg_label, \\\n",
        "                                          input_is_latent=True, noise=noise)\n",
        "            result.append(F.interpolate(fake_img.detach().cpu().clamp(-1.0, 1.0),(resolution_vis,resolution_vis)))\n",
        "\n",
        "        result = torch.cat(result, dim=0)\n",
        "        result = (utils.make_grid(result, nrow=nrows) + 1) / 2 * 255\n",
        "        result = result.numpy()[::-1].transpose((1, 2, 0)).astype('uint8')\n",
        "        out.write(result)\n",
        "        result = []\n",
        "        success, img = cap.read()\n",
        "        \n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L8uKwcaqtJU-",
      "metadata": {
        "id": "L8uKwcaqtJU-"
      },
      "outputs": [],
      "source": [
        "mp4 = open(save_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcyf_FH3tivM",
      "metadata": {
        "id": "fcyf_FH3tivM"
      },
      "outputs": [],
      "source": [
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xm7_b5YyqQoa",
      "metadata": {
        "id": "xm7_b5YyqQoa"
      },
      "source": [
        "# free-viewpoint protrait"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "glV_zc6wqQoa",
      "metadata": {
        "id": "glV_zc6wqQoa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from modules.sof.utils.seg_sampler import FaceSegSampler\n",
        "\n",
        "img_size = 128\n",
        "num_instances = 1000\n",
        "num_poses = 100\n",
        "sample_mode = 'spiral'\n",
        "radius = 4.5\n",
        "\n",
        "seg_sampler = FaceSegSampler(\n",
        "    model_path='./ckpts/epoch_0250_iter_050000.pth', \n",
        "    img_size=512, \n",
        "    sample_mode='spiral',\n",
        "    sample_radius=radius\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vclwo6hVqQob",
      "metadata": {
        "id": "Vclwo6hVqQob"
      },
      "outputs": [],
      "source": [
        "save_path = './example/fvv.mp4'\n",
        "resolution_vis = 512 # image resolution to save \n",
        "nrows,ncols = 2, 2\n",
        "width_pad, height_pad = 2 * (ncols + 1), 2 * (nrows + 1) \n",
        "n_feames = 120\n",
        "\n",
        "# sampling instance embedding\n",
        "smp_ins = torch.from_numpy(seg_sampler.gmm.sample(1)[0]).float()\n",
        "\n",
        "# sampling poses\n",
        "look_at = np.asarray([0, 0.1, 0.0])\n",
        "cam_center =  np.asarray([0, 0.1, 4.5])\n",
        "smp_poses = seg_sampler.sample_pose(\n",
        "    cam_center, look_at, \n",
        "    num_samples=n_feames, emb=smp_ins)\n",
        "print('Samping spiral poses: ', smp_poses.shape)\n",
        "\n",
        "# generate images\n",
        "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
        "out = cv2.VideoWriter(\n",
        "    f'{save_path[:-4]}_spiral.mp4', fourcc,\n",
        "    20, (resolution_vis * ncols + width_pad, resolution_vis * nrows + height_pad))\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_styles = nrows*ncols\n",
        "    seg_label = id_remap(torch.from_numpy(smp_poses[:1]).float()).to(device)\n",
        "    noise = [getattr(generator.noises, f'noise_{i}') for i in range(generator.num_layers)]\n",
        "    w_latents = sample_styles_with_miou(\n",
        "            seg_label, n_styles, mixstyle=0.0, truncation=args.truncation, batch_size=args.batch_size,descending=True)\n",
        "\n",
        "    try:\n",
        "        tqdm._instances.clear() \n",
        "    except Exception:     \n",
        "        pass\n",
        "    for seg_label in tqdm(smp_poses):\n",
        "        \n",
        "        seg_label = id_remap(torch.from_numpy(seg_label).float()[None,None]).to(device)\n",
        "        \n",
        "        seg_label_rgb = vis_condition_img(seg_label)\n",
        "        seg_label_rgb = F.interpolate(seg_label_rgb, (resolution_vis, resolution_vis), mode='bilinear', align_corners=True)\n",
        "        result = [seg_label_rgb]\n",
        "        for j in range(nrows*ncols-1):\n",
        "            fake_img, _, _, _ = generator(  w_latents[j], return_latents=False,\n",
        "                                            condition_img=seg_label, \\\n",
        "                                            input_is_latent=True, noise=noise)\n",
        "        \n",
        "            result.append(F.interpolate(fake_img.detach().cpu().clamp(-1.0, 1.0),(resolution_vis,resolution_vis)))\n",
        "\n",
        "        result = torch.cat(result, dim=0)\n",
        "        result = (utils.make_grid(result, nrow=nrows) + 1) / 2 * 255\n",
        "        result = result.numpy()[::-1].transpose((1, 2, 0)).astype('uint8')\n",
        "        out.write(result)\n",
        "\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_oQfW3EDuNKb",
      "metadata": {
        "id": "_oQfW3EDuNKb"
      },
      "outputs": [],
      "source": [
        "save_path = './example/fvv_spiral.mp4'\n",
        "mp4 = open(save_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q1qD8ao1uQpo",
      "metadata": {
        "id": "Q1qD8ao1uQpo"
      },
      "outputs": [],
      "source": [
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Copy of Sofgan renderer.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
